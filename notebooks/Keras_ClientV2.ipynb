{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import from parent directory.\"\"\"\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgymh/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "#from generic_model import GenericKerasModel\n",
    "from models.generic_model import GenericKerasModel\n",
    "\n",
    "class KerasPerceptron(GenericKerasModel):\n",
    "    def __init__(self, is_training=False):\n",
    "        self.n_input = 784\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 200\n",
    "        self.n_classes = 10\n",
    "        self.is_training = is_training\n",
    "        self.model = self.build_model()\n",
    "        if is_training:\n",
    "            self.compile_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_hidden1, input_shape=(self.n_input,), activation='relu'))\n",
    "        model.add(Dense(self.n_hidden2, activation='relu'))\n",
    "        model.add(Dense(self.n_classes, activation='linear'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        sgd = optimizers.SGD(lr=0.01, decay=1e-6)\n",
    "        self.model.compile(\n",
    "            optimizer=sgd,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc']\n",
    "        )\n",
    "\n",
    "def _create_dataset_iterator(dataset_path, max_count, iter_type, batch_size, labeler):\n",
    "    \"\"\"\n",
    "    Returns an iterator of batches of size B containing all features of the data.\n",
    "\n",
    "    Assumes `dataset_path` is a path to a folder with multiple CSV files.\n",
    "    \"\"\"\n",
    "    assert iter_type in ['train', 'test'], \"'iter_type' parameter is invalid.\"\n",
    "    if iter_type == 'train':\n",
    "        directories = os.listdir(dataset_path)\n",
    "    elif iter_type == 'test':\n",
    "        directories = reversed(os.listdir(dataset_path))\n",
    "\n",
    "    count = 0\n",
    "    for filename in directories:\n",
    "        if not filename.endswith(\".csv\"): continue\n",
    "        full_path = os.path.join(dataset_path, filename)\n",
    "        with open(full_path, 'r') as f:\n",
    "            batch = []\n",
    "            for line in f:\n",
    "                if count >= max_count:\n",
    "                    print('exit at {} and {}'.format(count, max_count))\n",
    "                    return\n",
    "                if len(batch) == batch_size:\n",
    "                    if batch_size == 1:\n",
    "                        yield batch[0]\n",
    "                    else:\n",
    "                        yield batch\n",
    "                    batch = []\n",
    "                line = labeler(line) if labeler else line\n",
    "                batch.append(line)\n",
    "                count += 1\n",
    "\n",
    "def create_train_dataset_iterator(dataset_path, count, split=0.8, batch_size=1, labeler=None):\n",
    "    \"\"\"\n",
    "    Returns an iterator of batches of size B containing all features of the data.\n",
    "\n",
    "    Assumes `dataset_path` is a path to a folder with multiple CSV files.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from _create_dataset_iterator(dataset_path, int(count*split), \"train\", \\\n",
    "            batch_size, labeler)\n",
    "        \n",
    "def create_test_dataset_iterator(dataset_path, count, split=0.2, batch_size=1, labeler=None):\n",
    "    \"\"\"\n",
    "    Returns an iterator of batches of size B containing all features of the data.\n",
    "\n",
    "    Assumes `dataset_path` is a path to a folder with multiple CSV files.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from _create_dataset_iterator(dataset_path, count*split, \"test\", \\\n",
    "            batch_size, labeler)\n",
    "\n",
    "def train_keras_model(serialized_model, weights, dataset_iterator, hyperparams):\n",
    "    logging.info('Keras training just started.')\n",
    "    assert weights != None, \"Initial weights must not be 'None'.\"\n",
    "    model = model_from_json(serialized_model)\n",
    "    model.set_weights(weights)\n",
    "    hist = model.fit_generator(dataset_iterator, epochs=hyperparams['epochs'])\n",
    "    new_weights = model.get_weights()\n",
    "    logging.info('Keras training complete.')\n",
    "    return new_weights, {'training_history' : hist.history}\n",
    "\n",
    "def mnist_labeler(line):\n",
    "    \"\"\"\n",
    "    Returns an (X, y) tuple from a line of the MNIST CSV files, where X is a\n",
    "    (784, 1) numpy array and y is a scalar.\n",
    "\n",
    "    The input is of the format `label, pix-11, pix-12, pix-13, ...`.\n",
    "    \"\"\"\n",
    "    line = line.split(',')\n",
    "    X = np.array(line[1:], dtype=np.uint8).reshape((784,))\n",
    "    y = int(line[0])\n",
    "    assert X.size == 784, \"Dimension of `X` is not 784.\"\n",
    "    assert 0 <= y <= 9, \"`y` is not between 0 and 9.\"\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    y = np.expand_dims(keras.utils.to_categorical(y, num_classes=10), axis=0)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/2\n",
      "47979/48000 [============================>.] - ETA: 0s - loss: 9.5273 - acc: 0.1046exit at 48000 and 48000\n",
      "48000/48000 [==============================] - 133s 3ms/step - loss: 9.5261 - acc: 0.1046\n",
      "Epoch 2/2\n",
      "47989/48000 [============================>.] - ETA: 0s - loss: 9.4581 - acc: 0.1047exit at 48000 and 48000\n",
      "48000/48000 [==============================] - 150s 3ms/step - loss: 9.4576 - acc: 0.1047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.1045625, 0.10466666666666667],\n",
       " 'loss': [9.526110862900659, 9.457628317661584]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count = 60000\n",
    "split = 0.8\n",
    "batch_size = 1\n",
    "epochs = 2\n",
    "\n",
    "m = KerasPerceptron(is_training=True)\n",
    "dataset_iterator = create_train_dataset_iterator('datasets/mnist', data_count, \\\n",
    "    split=split, batch_size=batch_size, labeler=mnist_labeler)\n",
    "\n",
    "# i = 1\n",
    "# for X, y in dataset_iterator:\n",
    "#     #print(len(b), len(b[0]), b[1])\n",
    "#     print(X)\n",
    "#     print(y)\n",
    "#     i -= 1\n",
    "#     if i == 0: exit(1)\n",
    "\n",
    "# i = 0\n",
    "# for b in _create_dataset_iterator('datasets/mnist', data_count*split, \\\n",
    "#     batch_size=batch_size, labeler=mnist_labeler):\n",
    "#     i += 1\n",
    "# i\n",
    "\n",
    "# i = 0\n",
    "# for b in dataset_iterator:\n",
    "#     i += 1\n",
    "# i\n",
    "\n",
    "hist = m.model.fit_generator(dataset_iterator, epochs=epochs, \\\n",
    "    steps_per_epoch=(data_count*split)//batch_size)\n",
    "hist.history\n",
    "\n",
    "# # m.build_optimizer(1.47)\n",
    "# # m.train(text, {\"epochs\":1, \"batch_size\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.1045625, 0.10466666666666667],\n",
       " 'loss': [9.526110862900659, 9.457628317661584]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.495692720710462, 0.1051754312859405]\n"
     ]
    }
   ],
   "source": [
    "dataset_iterator = create_test_dataset_iterator('datasets/mnist', data_count, \\\n",
    "    split=split, batch_size=batch_size, labeler=mnist_labeler)\n",
    "\n",
    "hist = m.model.evaluate_generator(dataset_iterator, steps=(data_count*(1-split))//batch_size)\n",
    "\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.1051754312859405, 'loss': 9.495692720710462}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(zip(m.model.metrics_names, hist))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
